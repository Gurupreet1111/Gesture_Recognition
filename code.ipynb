{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "mount_file_id": "1Zy5-iodxn-086YeHagR96966Lf7Q2gDZ",
      "authorship_tag": "ABX9TyP8JN98zPSJkdgKjKdLDeyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gurupreet1111/Gesture_Recognition/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7bEZpVbYCFd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f859b700-17f1-46e4-f62a-b437ef8f9814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Gesture_Recognition'...\n",
            "remote: Enumerating objects: 23532, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 23532 (delta 0), reused 6 (delta 0), pack-reused 23526\u001b[K\n",
            "Receiving objects: 100% (23532/23532), 1.58 GiB | 67.89 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "Updating files: 100% (22894/22894), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Gurupreet1111/Gesture_Recognition.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "RaqrmJnZ_5f3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from imageio.v2 import imread\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import datetime\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "#import tensorflow as tf\n",
        "# tf.set_random_seed(30) # deprecated\n",
        "#tf.random.set_seed(30)\n",
        "#import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "OP4dB-la6Q5t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "project_folder= \"/content/Gesture_Recognition/Project_data\""
      ],
      "metadata": {
        "id": "cIJ06jK64b7N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ii9kk4-5N7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_doc = np.random.permutation(open(f'{project_folder}/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open(f'{project_folder}/val.csv').readlines())\n",
        "batch_size = 100 #experiment with the batch size\n",
        "image_count = 30 # number of images to be used for each video\n",
        "image_height=160\n",
        "image_width=160\n",
        "channels=3\n",
        "num_classes=5 # number of classes\n",
        "input_shape=(image_count,image_width,image_height,channels)\n",
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = f'{project_folder}/train'\n",
        "val_path = f'{project_folder}/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 10 # choose the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftkPEagu6HLF",
        "outputId": "39bbbacd-e598-494f-a006-dd0e26630c5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx =[x for x in range(0,image_count)]\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches = len(t)//batch_size    # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size,image_count,image_width,image_height,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,num_classes)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "                    temp = cv2.resize(image,(image_width,image_height))\n",
        "                    temp = temp/255 #Normalize data\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])  #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])  #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])  #normalise and feed in the image\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if (len(folder_list) != batch_size*num_batches):\n",
        "            print(\"Batch: \",num_batches+1,\"Index:\", batch_size)\n",
        "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
        "            batch_data = np.zeros((batch_size,image_count,image_width,image_height,channels)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,num_classes)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    temp = cv2.resize(image,(image_width,image_height))\n",
        "                    temp = temp/255 #Normalize data\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = (temp[:,:,0])\n",
        "                    batch_data[folder,idx,:,:,1] = (temp[:,:,1])\n",
        "                    batch_data[folder,idx,:,:,2] = (temp[:,:,2])\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels\n"
      ],
      "metadata": {
        "id": "n6T3v_Bk6LTB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout,GlobalAveragePooling3D,GRU,GlobalAveragePooling2D,LSTM,MaxPool2D,ConvLSTM2D,GlobalAveragePooling1D\n",
        "from keras.layers import Conv3D, MaxPooling3D,Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.applications import MobileNet"
      ],
      "metadata": {
        "id": "UKORRQZH6cNL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import ResNet152V2\n",
        "# EfficientNetB7 with rnn gru units\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(ResNet152V2(weights='imagenet', include_top=False),input_shape=input_shape, name='ResNet152V2', trainable=False))\n",
        "model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
        "model.add(TimeDistributed(Dense(64, activation='relu')))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GRU(128, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE7cOn8E6e1R",
        "outputId": "678c852c-fb2d-4168-b28d-86b38d79fd27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234545216/234545216 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ],
      "metadata": {
        "id": "3i3-MJHX_hXQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode='auto')\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "metadata": {
        "id": "tLJrbFtO_r2z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = optimizers.Adam() #write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7W_XUoM_t3u",
        "outputId": "776a5e48-fa3c-43d5-c572-b73107347515"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " ResNet152V2 (TimeDistribut  (None, 30, 5, 5, 2048)    58331648  \n",
            " ed)                                                             \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 30, 2048)          0         \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 30, 64)            131136    \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 30, 256)           16640     \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 30, 256)           1024      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 30, 128)           148224    \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 30, 128)           512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3840)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               983296    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 1285      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 59613765 (227.41 MB)\n",
            "Trainable params: 1281349 (4.89 MB)\n",
            "Non-trainable params: 58332416 (222.52 MB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ],
      "metadata": {
        "id": "qiGIb5sy_0WO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(train_generator,\n",
        "                  steps_per_epoch=steps_per_epoch,\n",
        "                  epochs=num_epochs,\n",
        "                  verbose=1,\n",
        "                 # callbacks=checkpoint,\n",
        "                #  callbacks=callbacks_list,\n",
        "                  validation_data=val_generator,\n",
        "                  validation_steps=validation_steps,\n",
        "                  class_weight=None,\n",
        "                  workers=1,\n",
        "                  initial_epoch=0,\n",
        "                  max_queue_size=3000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97SnOelXABZK",
        "outputId": "0f65cce7-d5c3-46bd-87ef-3feded9b1b08"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/7 [===>..........................] - ETA: 10:29 - loss: 1.9986 - categorical_accuracy: 0.3200Batch:  7 Index: 100\n",
            "7/7 [==============================] - ETA: 0s - loss: 1.4968 - categorical_accuracy: 0.5109  Source path =  /content/Gesture_Recognition/Project_data/val ; batch size = 100\n",
            "7/7 [==============================] - 647s 90s/step - loss: 1.4968 - categorical_accuracy: 0.5109 - val_loss: 1.4269 - val_categorical_accuracy: 0.3200\n",
            "Epoch 2/10\n",
            "5/7 [====================>.........] - ETA: 2:17 - loss: 1.0444 - categorical_accuracy: 0.7175Batch:  11 Index: 63\n",
            "7/7 [==============================] - 538s 78s/step - loss: 1.1150 - categorical_accuracy: 0.7098 - val_loss: 1.4810 - val_categorical_accuracy: 0.3500\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 317s 47s/step - loss: 0.4401 - categorical_accuracy: 0.8442 - val_loss: 1.5319 - val_categorical_accuracy: 0.4600\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 316s 47s/step - loss: 0.5708 - categorical_accuracy: 0.8139 - val_loss: 1.3927 - val_categorical_accuracy: 0.4800\n",
            "Epoch 5/10\n",
            "5/7 [====================>.........] - ETA: 1:14 - loss: 0.6976 - categorical_accuracy: 0.7879Batch:  21 Index: 33\n",
            "7/7 [==============================] - 319s 47s/step - loss: 0.6986 - categorical_accuracy: 0.7922 - val_loss: 1.3010 - val_categorical_accuracy: 0.4600\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 80s 13s/step - loss: 2.3868 - categorical_accuracy: 0.5238 - val_loss: 2.6049 - val_categorical_accuracy: 0.3700\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 80s 13s/step - loss: 3.8530 - categorical_accuracy: 0.4286 - val_loss: 7.8602 - val_categorical_accuracy: 0.2100\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 80s 13s/step - loss: 5.5242 - categorical_accuracy: 0.4286 - val_loss: 3.8164 - val_categorical_accuracy: 0.3700\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 80s 13s/step - loss: 3.3514 - categorical_accuracy: 0.3333 - val_loss: 6.6012 - val_categorical_accuracy: 0.2800\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 80s 13s/step - loss: 3.4941 - categorical_accuracy: 0.4286 - val_loss: 5.4263 - val_categorical_accuracy: 0.4100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gKIABuey1NRv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5h28YtxkxlO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}